{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbT3SApH0bn7"
      },
      "source": [
        "# Load required things and setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84m73cj0zYi4"
      },
      "outputs": [],
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/cuda/112/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl --force-reinstall "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0VZBFbTlCR-",
        "outputId": "4a844bfe-b027-42f1-b064-c94039ca6b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "7rGU5scXqEbp",
        "outputId": "c9a0978d-30bd-411f-b3bd-dbb21cd5d845"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b6401a6fdf46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datetime import datetime\n",
        "import psutil \n",
        "import matplotlib.pyplot as plt\n",
        "import pdb\n",
        "from sklearn.metrics import f1_score\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "from google.colab import drive\n",
        "import gc\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-Ip-3YLpFJn",
        "outputId": "dc38f567-0e52-4986-b3c5-d381295f119c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xla:1\n",
            "USING TPU\n",
            "False\n",
            "xla:1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "CHUNK_SIZE = 100000#data points per file\n",
        "MAX_TOKEN_DIM = 512 #controls padding and input to classifier\n",
        "LOAD_DATA = True\n",
        "PCA_DIM = 128\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  torch.cuda.device(device)\n",
        "try:\n",
        "  print(xm.xla_device())\n",
        "  device = xm.xla_device()\n",
        "  print(\"USING TPU\")\n",
        "except:\n",
        "  print(\"NOT USING TPU\")\n",
        "  pass\n",
        "print(torch.cuda.is_available())\n",
        "print(device)\n",
        "if torch.cuda.is_available():\n",
        "  torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "  print(\"using cuda\")\n",
        "  \n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
        "codebert = AutoModel.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
        "torch.set_printoptions(precision=7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhrTHlM6pOfa"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "  f = open(\"/content/drive/MyDrive/CloneData/data.jsonl\") #read sniipets and indices\n",
        "  entries = f.readlines()\n",
        "  objects = [json.loads(x) for x in entries] #load all functions\n",
        "  idx_to_function = dict() #id num -> code snippiet\n",
        " \n",
        "  for snippet in objects:#map to associate index to func\n",
        "    \n",
        "    idx_to_function[snippet[\"idx\"]] = snippet[\"func\"]\n",
        "\n",
        "  return idx_to_function #map id num to code 0 -> \"hello world\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiomZTWIp0rQ"
      },
      "outputs": [],
      "source": [
        "def pairify_file(lines : list, idx_to_function : dict) -> tuple:\n",
        "  #list of lines id1 id2 label\n",
        "  # (code1, code2, label)\n",
        "  examples = [] #list of lines of \n",
        "  \n",
        "  for line in lines:\n",
        "    line_entries = line.replace(\"\\t\", \" \").split(\" \") #given line x y label, divide to find if x is y according to label\n",
        "    #print(line)\n",
        "    x = line_entries[0]\n",
        "    y = line_entries[1]\n",
        "    label = line_entries[2]\n",
        "    \n",
        "    examples.append((idx_to_function[x], idx_to_function[y], float(label))) #convert label to float for pytorch\n",
        "  return examples # [(\"hello world\", \"hi wurld\", 1) , (\"bye world\", \"EEEEEEE\", 0)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49GUPqpspx2P"
      },
      "outputs": [],
      "source": [
        "def split_and_label_data(idx_to_function : dict): #convert pairs to useful training examples\n",
        "  return tuple(map(  lambda x : pairify_file(open(x).readlines(), idx_to_function)  , [\"/content/drive/MyDrive/CloneData/train.txt\",\"/content/drive/MyDrive/CloneData/test.txt\", \"/content/drive/MyDrive/CloneData/valid.txt\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EYrKogx0kQo"
      },
      "source": [
        "# Pre-calculate embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLJLCUu4w0PY"
      },
      "outputs": [],
      "source": [
        "def embed(x : str) -> torch.TensorType:\n",
        "  with torch.no_grad():\n",
        "    code_tokens=tokenizer.tokenize(x)\n",
        "\n",
        "    if len(code_tokens) >= 510: #confirm tokes arent too big for model\n",
        "      return None\n",
        "    tokens=[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
        "\n",
        "    tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "    context_embeddings=codebert(torch.tensor(tokens_ids, device = device)[None,:])[0]\n",
        "    \n",
        "    flattened = torch.flatten(context_embeddings)\n",
        "    \n",
        "    \n",
        "    \n",
        "    return flattened #torch.clamp(flattened, min = -2, max = 2) #return flattened embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggTEK92rBaLQ"
      },
      "outputs": [],
      "source": [
        "def embed_data(data : list) -> list: #takes prog1, prog2, label and replaces prog with their embedding for every item in the list and filters out too long items\n",
        "  embedded_data = []\n",
        "  i = 0\n",
        "  \n",
        "  for x,y, label in data:\n",
        "\n",
        "    if i % 10 == 0:  \n",
        "      #print(\"using {} MB for {} of {}, embedded {}\".format(psutil.Process().memory_info().rss / (1024 * 1024),i, len(data), len(embedded_data)))\n",
        "      pass\n",
        "    emb_x = embed(x[0])\n",
        "    emb_y = embed(y[0])\n",
        "    #pdb.set_trace()\n",
        "    if emb_x != None and emb_y != None: #check code isnt too long\n",
        "      x_embed = emb_x #Standardize embeddings lengths since they depend on #of tokens\n",
        "      y_embed = emb_y\n",
        "     \n",
        "      padding_length_x  = (MAX_TOKEN_DIM * 768 - x_embed.size()[0])\n",
        "      padding_length_y  = (MAX_TOKEN_DIM * 768 - y_embed.size()[0])\n",
        "      \n",
        "      x_padded = torch.nn.functional.pad(x_embed, (int(padding_length_x/2), int(padding_length_x/2)))\n",
        "      y_padded = torch.nn.functional.pad(y_embed, (int(padding_length_y/2), int(padding_length_y/2)))\n",
        "      embedded_data.append((x_padded,y_padded, label))\n",
        "    i += 1\n",
        "  #print(f\"unique entries {len(set([embedded_data[0][0],  embedded_data[1][0]]  ))}\")\n",
        "  return embedded_data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00CK6wwm9MjH"
      },
      "outputs": [],
      "source": [
        "class CloneDataset(Dataset): #dataset \n",
        "\n",
        "  def __init__(self,x : list ,y : list,labels : list):\n",
        "    assert len(x) == len(y) and len(y) == len(labels) #make sure all the same size\n",
        "    #standard boilerplate\n",
        "    self.x = x.to(device)\n",
        "    self.y = y.to(device)\n",
        "    self.labels = labels.to(device)\n",
        "    self.length = len(x)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx], self.labels[idx]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hafzrTWspZ1P"
      },
      "outputs": [],
      "source": [
        "\n",
        "#test_data = embed_data(test_data)\n",
        "#validation_data = embed_data(validation_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bz8fusDHb6S7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "6655eb94-1910-46ac-d1b2-ba029c7ef1f3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-408e6f927784>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    tokens=[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]tokens\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def tokenize(code : str) -> list:\n",
        "  with torch.no_grad():\n",
        "    code_tokens=tokenizer.tokenize(code)\n",
        "    \n",
        "\n",
        "    if len(code_tokens) >= 510: #confirm tokes arent too big for model\n",
        "      return None\n",
        "    code_tokens += [tokenizer.pad_token] *  (510 - len(code_tokens)) #pad out to 510 which becomes 512\n",
        "    tokens=[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]tokens\n",
        "\n",
        "    tokens_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "    return torch.tensor(tokens_ids, device = device)[None,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9GsM8wgAXRT"
      },
      "outputs": [],
      "source": [
        "def build_dataset(data : list):\n",
        "  x_list = []\n",
        "  y_list = []\n",
        "  label_list = []\n",
        "  i = 0\n",
        "  for x,y,l in data:#convert list of tuples to 3 separate lists\n",
        "    #x.to(device)\n",
        "    #y.to(device)\n",
        "    if i % 250 == 0:\n",
        "        print(f\"on data point {i}\")\n",
        "    x_tokens = tokenize(x)\n",
        "    y_tokens = tokenize(y)\n",
        "    if  not x_tokens is None and not y_tokens is None: #confirmm both seqs work\n",
        "      x_list.append(x_tokens)\n",
        "      y_list.append(y_tokens)\n",
        "      label_list.append(l)\n",
        "    i+=1\n",
        "\n",
        "  return CloneDataset(x_list, y_list, label_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZZu9vOmt8tm"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "       \n",
        "        \n",
        "        #A note on architecture for those interested, we eat CodeBERT embeddings of size X  * 768 which have been flattened\n",
        "        # Now those vectors are each fed into FF layer(s)\n",
        "        #Then they're concatnated and fed thru more FF layer(s)\n",
        "        # Then their dimensionality is shrunk down to 1, which is sigmoided\n",
        "        input_size = 512\n",
        "        layer2_size = 256\n",
        "        layer3_size = 128\n",
        "        layer4_size = 32\n",
        "        #self.xlayer_1 = nn.Linear(MAX_TOKEN_DIM * 768, layer2_size)\n",
        "        #self.ylayer_1 = nn.Linear(MAX_TOKEN_DIM * 768, layer2_size)\n",
        "        self.reduction = nn.Linear(MAX_TOKEN_DIM * 768, input_size)\n",
        "        self.ff1 = nn.Linear( 2 * input_size, layer2_size )\n",
        "        \n",
        "        self.ff2 = nn.Linear(layer2_size, 1)\n",
        "        #self.ff3 = nn.Linear(layer3_size, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.batchNorm1 = nn.BatchNorm1d(layer2_size, affine = False)\n",
        "        #self.batchNorm2 = nn.BatchNorm1d(layer3_size, affine = False)\n",
        "\n",
        "        #nn.init.xavier_normal_(self.xlayer_1.weight)\n",
        "        #nn.init.xavier_normal_(self.ylayer_1.weight)\n",
        "        nn.init.xavier_normal_(self.ff1.weight)\n",
        "        nn.init.xavier_normal_(self.ff2.weight)\n",
        "        nn.init.xavier_normal_(self.reduction.weight)\n",
        "        #nn.init.xavier_normal_(self.ff3.weight)\n",
        "\n",
        "        #self.to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x,y):\n",
        "       x_reduced = self.relu(self.reduction(x))   \n",
        "       y_reduced = self.relu(self.reduction(y))       \n",
        "       combined = torch.cat((x_reduced, y_reduced),1)\n",
        "      \n",
        "       out = self.ff1(combined)\n",
        "       #print(f\"out is {out}\")\n",
        "       \n",
        "       out = self.sigmoid(out)\n",
        "       #xm.mark_step()\n",
        "       out = self.batchNorm1(out)\n",
        "       #xm.mark_step()\n",
        "       out = self.ff2(out)\n",
        "       #out = self.sigmoid(out)\n",
        "       #out = self.batchNorm2(out)\n",
        "       #xm.mark_step()\n",
        "       #out = self.relu(out)\n",
        "       #out = self.ff3(out)\n",
        "       \n",
        "       #out = self.sigmoid(out)\n",
        "       return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk9DLArP5sWq",
        "outputId": "14c290a7-81f2-48b8-d901-49884b921fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000\n"
          ]
        }
      ],
      "source": [
        "print(CHUNK_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save((torch.tensor([1,2,3]) ), \"test.pt\")\n",
        "torch.load(\"test.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlG-lUXbrIVM",
        "outputId": "cff4b022-9278-49a5-bd10-01201f4d45ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6LWC8lU0SGU",
        "outputId": "0dff460e-b372-4c60-9185-870514c210d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading file 0\n",
            "xla:1\n",
            "took 0:00:06.518861 s\n",
            "loading file 1\n",
            "xla:1\n",
            "took 0:00:05.642586 s\n",
            "loading file 2\n",
            "xla:1\n",
            "took 0:00:05.775386 s\n",
            "loading file 3\n",
            "xla:1\n",
            "took 0:00:05.841307 s\n",
            "loading file 4\n",
            "xla:1\n",
            "took 0:00:05.502941 s\n",
            "loading file 5\n",
            "xla:1\n",
            "took 0:00:06.163734 s\n",
            "loading file 6\n",
            "xla:1\n",
            "took 0:00:05.856707 s\n",
            "loading file 7\n",
            "xla:1\n",
            "took 0:00:05.740783 s\n",
            "loading file 8\n",
            "xla:1\n",
            "took 0:00:05.774251 s\n",
            "loading file 9\n",
            "xla:1\n",
            "took 0:00:05.779758 s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "hector = False\n",
        "train_data = []\n",
        "if not LOAD_DATA:\n",
        "  #START\n",
        "  idx_to_function = load_data()\n",
        "  train_data, test_data,validation_data = split_and_label_data(idx_to_function)\n",
        "  if hector:\n",
        "    train_data = train_data[:100000]\n",
        "  i = 0\n",
        "  while i * CHUNK_SIZE < len(train_data):\n",
        "    relevant_data = train_data[i * CHUNK_SIZE: min(len(train_data), (i+1) * CHUNK_SIZE ) ] #get 100000 items at a time\n",
        "    print(i * CHUNK_SIZE, min(len(train_data), (i+1) * CHUNK_SIZE ), len(relevant_data))\n",
        "    print(f\"building dataset {i}\")\n",
        "    dataset = build_dataset(relevant_data) #get and save data\n",
        "    \n",
        "    print(f\"saving dataset{i}\")\n",
        "    torch.save(dataset, f\"train_data_{i}.pt\")\n",
        "    i+=1 #\n",
        "\n",
        "  #END\n",
        "elif LOAD_DATA:  \n",
        "  data_loaders = []\n",
        "  for i in range(10):#41 mins\n",
        "    print(f\"loading file {i}\")\n",
        "    start  = datetime.now()\n",
        "    print(device)\n",
        "    dl = torch.load(f\"/content/drive/MyDrive/CloneData/id_data/train_data_{0}.pt\", map_location= \"cpu\")\n",
        "    \n",
        "    data_loaders.append(dl)\n",
        "    end = datetime.now()\n",
        "    print(f\"took {(end-start)} s\")\n",
        "  train_data = data_loaders\n",
        "  train_data = torch.utils.data.ConcatDataset(data_loaders)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUqf_FPNTSKa"
      },
      "outputs": [],
      "source": [
        "trainLoader = DataLoader(train_data, batch_size= 256, shuffle = False, drop_last = True)# BATCH SIZES MUST BE MULTIPLES OF 128\n",
        "all_train = DataLoader(train_data, batch_size= len(train_data), shuffle = False, drop_last = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKE-wWUmP201",
        "outputId": "4b7cd34b-86f3-4d56-dc72-04d3514cf918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "for x,y,l in all_train:\n",
        "  print(type(x),type(y), type(l))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9hAA5MwQMIS"
      },
      "outputs": [],
      "source": [
        "#torch.save(train_data, \"train_data_0.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCv4hn8xQTLa"
      },
      "outputs": [],
      "source": [
        "#del train_data\n",
        "#train_data = torch.load(\"train_data.pt\")\n",
        "#print(train_data, \"Fdsfasd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf1LORXo3kIz"
      },
      "outputs": [],
      "source": [
        "class F1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(F1, self).__init__()\n",
        "       \n",
        "\n",
        "\n",
        "    def forward(self, label,pred):\n",
        "      with torch.no_grad():\n",
        "        x = torch.round(pred) - label\n",
        "        x  = torch.flatten(x)\n",
        "        tp = torch.where(x == 0, 1, 0)\n",
        "        tp_count = float(torch.numel(torch.nonzero(tp)))\n",
        "\n",
        "        fp = torch.where(x == 1, 1, 0)\n",
        "        fp_count = float(torch.numel(torch.nonzero(fp)))\n",
        "\n",
        "        fn = torch.where(x == -1, 1, 0)\n",
        "        fn_count = float(torch.numel(torch.nonzero(fn)))\n",
        "        denom = (tp_count + .5 * (fp_count + fn_count))\n",
        "        #print(tp_count, fp_count, fn_count)\n",
        "        #print(denom)\n",
        "        if denom == 0:\n",
        "          return 0\n",
        "        return float(tp_count) / float(denom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXLGEVbK0qda"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDmyDuz6wuN7"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  torch.cuda.empty_cache() \n",
        "  epochs  = 2 #standard boilerplate\n",
        "  model = Classifier()\n",
        "  print(model.ff1.weight.device)\n",
        "  #criterion = nn.BCELoss()\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  #optimizer = optim.Adam(model.parameters())\n",
        "  optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
        "  #scorer = F1()\n",
        "  #scorer.to(device)\n",
        "\n",
        "  loss_history = []\n",
        "  f1_history = []\n",
        "  max_grad_history = []\n",
        "  min_grad_history = []\n",
        "  print(device)\n",
        "  model.to(device)\n",
        "  codebert.to(device)\n",
        "  U,S,V = None, None, None\n",
        "  for epoch in range(epochs): #standard training procedure\n",
        "    torch.cuda.empty_cache() \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    tp_count = 0 #setup for f1 score\n",
        "    fp_count = 0\n",
        "    fn_count = 0\n",
        "    f1 = 0\n",
        "    \n",
        "    j = 0\n",
        "    for x,y,label in trainLoader:\n",
        "      if j > 20:\n",
        "        break\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()  \n",
        "      \n",
        "\n",
        "\n",
        "      with torch.no_grad():\n",
        "        x = torch.reshape(x, (x.shape[0], x.shape[2])) # make Batch size X 1 X 512 into Batch size X 512\n",
        "        y = torch.reshape(y, (y.shape[0], y.shape[2]))\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        #model.to(\"cpu\")\n",
        "        #codebert.to(device)\n",
        "        #torch.cuda.empty_cache()\n",
        "        #print(f\"before embedding codebert is on {codebert.device}\")\n",
        "        embed_start = datetime.now()\n",
        "        embedded_x=codebert(x)[0]\n",
        "       \n",
        "        embedded_y=codebert(y)[0]\n",
        "       \n",
        "      \n",
        "        #codebert.to(\"cpu\")\n",
        "        #model.to(device)\n",
        "        #torch.cuda.empty_cache()\n",
        "        #print(embedded_x.shape)\n",
        "        #print(embedded_y.shape)\n",
        "        #embedded_x.to('cpu')\n",
        "        #embedded_y.to('cpu')\n",
        "        \n",
        "        embedded_x = torch.flatten(embedded_x, start_dim = 1)\n",
        "        embedded_y = torch.flatten(embedded_y, start_dim = 1)\n",
        "        embed_end = datetime.now()\n",
        "        if j == 0 and epoch == 0:\n",
        "          print(\"before PCA\")\n",
        "          _,_,V = torch.pca_lowrank(embedded_x, q=128, center=True, niter=2)\n",
        "          print(\"got v\")\n",
        "          V = torch.transpose(V,0,1 )\n",
        "          print(\"reshaped v\")\n",
        "        print(embedded_x.shape, V.shape)\n",
        "        embedded_x = torch.reshape(embedded_x, embedded_x.shape)\n",
        "        print(\"mult by v\")\n",
        "        embedded_x =  embedded_x *   V\n",
        "        embedded_y = embedded_y * V\n",
        "        #embedded_x.to(device)\n",
        "        #embedded_y.to(device)\n",
        "        #print(embedded_x.shape)\n",
        "        #print(embedded_y.shape)\n",
        "       \n",
        "      \n",
        "      model_start = datetime.now()\n",
        "      optimizer.zero_grad()\n",
        "      #print(embedded_x.dtype)\n",
        "      #print(\"max x {}\".format(torch.max(embedded_x)))\n",
        "      #print(f\"before prediction model is on device {model.ff1.weight.device}\")\n",
        "      pred = model(embedded_x,embedded_y)\n",
        "      \n",
        "     \n",
        "      #print(f\"pred is {pred.shape} {pred}\")\n",
        "     \n",
        "\n",
        "\n",
        "      #print(label.shape)\n",
        "      #print(pred.view(10).shape)\n",
        "      loss_start = datetime.now()\n",
        "      loss = criterion(torch.flatten(pred.unsqueeze(1)),torch.flatten(label.unsqueeze(1)))\n",
        "      #loss = torch.nn.functional.binary_cross_entropy_with_logits(torch.flatten(pred.unsqueeze(1)),torch.flatten(label.unsqueeze(1)))\n",
        "      loss.backward()\n",
        "      #print(model.ff1.weight.grad)\n",
        "      #nn.utils.clip_grad_norm_(model.parameters(), max_norm = 2.0, norm_type = 2.0)\n",
        "      optimizer.step()\n",
        "      min_grad_history.append(torch.min(model.ff1.weight.grad).detach())\n",
        "      max_grad_history.append(torch.max(model.ff1.weight.grad).detach())\n",
        "      xm.mark_step()\n",
        "      \n",
        "      loss_end = datetime.now()\n",
        "      #print(\"pred is {}\".format(pred))\n",
        "      epoch_loss += 0 #loss.item()\n",
        "      end = datetime.now()\n",
        "      model_delta_t = end-model_start \n",
        "      embed_delta_t = embed_end-embed_start\n",
        "      loss_delta_t = loss_start-loss_end\n",
        "     \n",
        "     \n",
        "      with torch.no_grad():\n",
        "        #f1_score = scorer(label,pred)\n",
        "        #F1_score = f1_score(label.cpu(), torch.round(pred).cpu())\n",
        "        #xm.mark_step()\n",
        "        score_start = datetime.now()\n",
        "        \n",
        "        #f1_history.append(F1_score)\n",
        "        if j % 1 == 0:\n",
        "          l = loss.item()\n",
        "          #print(torch.min(pred), torch.max(pred))\n",
        "          print(\"time per model iteration {} s\".format(model_delta_t.microseconds / 10**6))\n",
        "          print(\"time per embed iteration {} s\".format(embed_delta_t.microseconds / 10**6))\n",
        "          print(\"time per loss iteration {} s\".format(loss_delta_t.microseconds / 10**6))\n",
        "          loss_history.append(l)\n",
        "          print(f\"iteation {j} of epoch {epoch+1}\")\n",
        "          print(f\"loss is {l}\")\n",
        "          \n",
        "        \n",
        "          #calculate scores\n",
        "          pred_cpu = pred.detach().cpu()\n",
        "          pred_rounded = torch.round(pred_cpu)\n",
        "          print(torch.min(pred_cpu), torch.max(pred_cpu))\n",
        "\n",
        "          for i in range(label.shape[0]):\n",
        "            #print(f\"i is {i}\")\n",
        "            if pred_rounded[i] == 1 and label[i] == 1:\n",
        "              tp_count += 1\n",
        "            elif pred_rounded[i] == 1 and label[i] == 0:\n",
        "              fp_count += 1\n",
        "            elif pred_rounded[i] == 0 and label[i] == 1:\n",
        "              fn_count += 1\n",
        "\n",
        "          if (tp_count + .5 * (fp_count + fn_count)) != 0: #dont get 0 for denom of f1\n",
        "            f1 = tp_count/(tp_count + .5 * (fp_count + fn_count))\n",
        "\n",
        "            #loss = loss.item.to('cpu')\n",
        "            \n",
        "            f1_history.append(f1)\n",
        "            score_end = datetime.now()\n",
        "            print(f\"{(score_end-score_start)} s for scoring\")\n",
        "        j+=1\n",
        "  return (loss_history,f1_history, min_grad_history, max_grad_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Gsr8-zrrzR7_",
        "outputId": "ac2a2c10-79a0-4082-a5b0-15230c55e60c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "xla:1\n",
            "before PCA\n",
            "got v\n",
            "reshaped v\n",
            "torch.Size([256, 393216]) torch.Size([128, 393216])\n",
            "mult by v\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-e4a2585adf8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_grad_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_grad_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-cf977560ee07>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0membedded_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mult by v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0membedded_x\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mV\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0membedded_x\u001b[0m \u001b[0;34m*\u001b[0m   \u001b[0membedded_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0membedded_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded_y\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m#embedded_x.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: (128, 393216) and (128, 393216)"
          ]
        }
      ],
      "source": [
        "loss_history, f1_history, min_grad_history,max_grad_history = train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch_xla.core.xla_model.get_memory_info(device))"
      ],
      "metadata": {
        "id": "UgDLK9e-IaX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for x,y,label in trainLoader:\n",
        "  if i > 5:\n",
        "    break\n",
        "  with torch.no_grad():\n",
        "        x = torch.reshape(x, (x.shape[0], x.shape[2])) # make Batch size X 1 X 512 into Batch size X 512\n",
        "        y = torch.reshape(y, (y.shape[0], y.shape[2]))\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        #model.to(\"cpu\")\n",
        "        #codebert.to(device)\n",
        "        #torch.cuda.empty_cache()\n",
        "        #print(f\"before embedding codebert is on {codebert.device}\")\n",
        "        embed_start = datetime.now()\n",
        "        embedded_x=codebert(x)[0]\n",
        "       \n",
        "        embedded_y=codebert(y)[0]\n",
        "       \n",
        "        embed_end = datetime.now()\n",
        "        #codebert.to(\"cpu\")\n",
        "        #model.to(device)\n",
        "        #torch.cuda.empty_cache()\n",
        "        #print(embedded_x.shape)\n",
        "        #print(embedded_y.shape)\n",
        "        #embedded_x.to('cpu')\n",
        "        #embedded_y.to('cpu')\n",
        "        \n",
        "        embedded_x = torch.flatten(embedded_x, start_dim = 1)\n",
        "        embedded_y = torch.flatten(embedded_y, start_dim = 1)\n",
        "        U,S,V = torch.pca_lowrank(embedded_x, q=None, center=True, niter=2)\n",
        "        print(S)\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "xkn8QDgUrvgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-4-FxV90s_9"
      },
      "source": [
        "# Analzye results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hDwOnK2p_AY"
      },
      "outputs": [],
      "source": [
        "plt.scatter(list(range(len(f1_history))), f1_history)\n",
        "plt.title(\"F1 score\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"F1\")\n",
        "plt.show()\n",
        "plt.scatter(list(range(len(loss_history))), loss_history)\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "plt.scatter(list(range(len(loss_history))), [x.detach().cpu() for x in  min_grad_history])\n",
        "plt.title(\"Min Grad\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Min Grad\")\n",
        "plt.show()\n",
        "plt.scatter(list(range(len(loss_history))), [x.detach().cpu() for x in  max_grad_history])\n",
        "plt.title(\"Max Grad\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Max Grad\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "codebertsimilar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}